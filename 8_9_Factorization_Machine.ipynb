{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_9_Factorization_Machine.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNhljzU2qOoRIoGuw/8UBHP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlskawns/RecSys_and_Retrieval_Study/blob/main/8_9_Factorization_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factorization Machine\n",
        "\n",
        "기존의 MF는 유저의 meta data나 아이템의 meta data를 활용하지 못함  \n",
        "유저 정보와 아이템의 정보(Auxiliary Features)를 넣으면 더더욱 개인화된 추천이 가능할 것으로 보임  \n",
        "\n"
      ],
      "metadata": {
        "id": "TUA2WKlyP0Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Factorization 이란? = Embedding\n",
        "\n",
        "MF의 기본 원리는 사용자, 유저에 대한 Latent Factor를 구하는 것.  \n",
        "Factorization Machine도 그 방법의 일종으로 볼 수 있음\n",
        "\n",
        "* SVM은 비선형 데이터셋에서도 높은 성능을 보여 가장 보편적인 모델\n",
        "* CF 환경(유저-아이템 상호작용)에선 sparse하기에 SVM이 성능이 좋지 못함 \n",
        "* 선형적으로 전반적인 Features를 모두 넣어서 활용도 할 수 있음\n"
      ],
      "metadata": {
        "id": "e6r2RXJJSIuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FM의 장점\n",
        "\n",
        "* 선형 복잡도를 가져 수십억 개 트레이닝 데이터셋도 빠르게 학습 가능\n",
        "* Supervised Learning 이므로 Regression, Classification, Ranking에 모두 활용 가능함"
      ],
      "metadata": {
        "id": "UH7WgnjJjz8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FM의 원리\n",
        "기존 MF의 방식은 버리고 새롭게 만든다.  \n",
        "필요한 features에 대해 one hot 인코딩을 진행해 features 정보를 넣는 방법\n",
        "* 데이터셋의 모양부터 MF가 아닌 일단 데이터프레임 형태로 변경\n",
        "* 각 필요한 meta data의 one hot encoding을 통해 feature를 생성한 뒤에 이에 대한 예상 평점을 **선형회귀**로 파악하는 방법\n",
        "\n",
        "* 일반 선형 회귀 : $\\hat y(x) = w_0 + \\displaystyle\\sum_{i=1}^n w_ix_i$\n",
        "  * $x_i$는 변수\n",
        "  * $w_i$는 학습 파라미터\n",
        "  * 변수의 개수는 1 ~ n\n",
        "  * 모든 변수$x_i$가 독립이어야 함 = 변수간의 interaction을 무시 -> FM에서는 변수들이 항상 독립일 순 없음(feature engineering 등)그렇게 될 수 없음\n",
        "\n",
        "* Interaction을 고려한 다항 선형 회귀 : $\\hat y(x) = w_o +\\displaystyle\\sum_{i=1}^nw_ix_i + \\displaystyle\\sum_{i=1}^n\\displaystyle\\sum_{j=i+1}^nw_{ij}x_ix_j$\n",
        "  * $w_{ij}$ 처럼 '남자'이면서 학력이 '대졸'인 사람의 어떠한 특징(interaction /bw features)도 고려하는 것 \n",
        "  * 파라미터 개수가 어마어마하게 많아지게 된다. \n",
        "  * 과적합 될 가능성도 많음\n",
        "\n"
      ],
      "metadata": {
        "id": "CnFJ153AzuEF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkjadV3rWSBj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}