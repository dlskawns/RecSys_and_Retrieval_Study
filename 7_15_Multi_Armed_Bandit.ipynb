{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_15_Multi_Armed_Bandit.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP6T/Ae/cpdanTJVg5T47Xk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlskawns/RecSys_and_Retrieval_Study/blob/main/7_15_Multi_Armed_Bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Armed Bandit\n",
        "\n",
        "Exploitation(착취)와 Exploration(탐험)을 활용한 강화학습의 원리를 이용한 추천입니다.\n",
        "\n",
        "실제 비즈니스에서 매우 많이 활용하는데, 카카오와 네이버에서도 이 방식을 많이 활용했다고 합니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W10_se3ek_te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAB 개요\n",
        "\n",
        "여러개의 선택지 중 어떤 것을 선택할지 그 방식을 고르는 추천 알고리즘.  \n",
        "\n",
        "카지노의 K개 슬롯머신을 N번 플레이 할 수 있는 상황. 각각의 슬롯머신은 당첨(1 or 0) 확률이 다를 때 수익을 최대화 하기 위한 선택의 순서를 찾는 것.\n",
        "\n",
        "\n",
        "\n",
        "#### Exploration(탐험): 정보를 더 얻기 위해 계속해서 새로운 선택지를 선택하는 것\n",
        "* 이것이 너무 적으면 더 잘 당첨될 수 있는 선택지를 포기하게 될 수 있습니다.\n",
        "* 너무 많을 경우, reward에 비해 비용이 많이 나가게 됩니다.\n",
        "#### Exploitation(착취): 현재까지의 기록으로 보아 가장 많이 나온 것을 선택하는 것\n",
        "* 탐험과의 trade off가 있고, 이것을 최소화 하는 것이 MAB의 포인트입니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WW1duBQgCJ_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAB 문제\n",
        "\n",
        "### MAB의 방식\n",
        "* $q*(a) \\dot= \\mathbb{E}[R_t|A_t = a]$\n",
        "  * $t$: 타임스텝 또는 횟수\n",
        "  * $A_t$: 타임스텝 t에 진행한 행동 number (슬롯머신의 번호)\n",
        "  * $R_t$: 타임스텝 t에 진행한 행동 $A_t$의 액션 $a$의 리워드(0, 1)\n",
        "  * $q*(a)$: 이러한 액션 $a$의 reward의 실제 기대값\n",
        "\n",
        "* 여기서 $q*(a)$를 추정할 때, 추정 가치가 최대인 action($a$)를 선택해 추정값 $Q_t(a)$를 정밀하게 하는 것\n",
        "  * Greedy Action: 전체 선택지에 대해 n번 시행 중 가장 많이 reward 나온 선택지를 선택하는 것\n",
        "    * Exploitation(착취)\n",
        "  * Other Action: 그 외 선택지를 선택\n",
        "    * Exploration(탐험)"
      ],
      "metadata": {
        "id": "tTT36bdxMM5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAB 종류\n",
        "#### Simple Average Method (greedy)\n",
        "단순 평균 값을 통해 기댓값 $q*(a)$를 아주 간단하게 추정하는 방식\n",
        "* 평균값에 따라 가장 추정가치가 높은 action을 선택하는 greedy algorithm\n",
        "  * $A_t = argmaxQ_t(a)$\n",
        "\n",
        "* $Q_t(a) \\dot= \\frac {타임스텝 t 이전에a의 reward의합}{타임스텝t이전의 a까지의 시도 수} = \\frac {\\sum_{i=1}^{t-1}R_i\\cdot\\mathbb{1}_A=a}{}$"
      ],
      "metadata": {
        "id": "dQE3kjVZ-zr6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgPkaxiVk-3Y"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}